{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d3001-68c9-4943-8cc4-dd1e319559c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langgraph langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01524fa8-e8a6-4264-ab18-140b868215e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Literal, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "import uuid\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b533c-3217-46a0-b0ad-6dc233398823",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to book a flight for a user based on user input.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "1. What the departure city is\n",
    "2. What the arrival city is\n",
    "3. What the date of travel is \n",
    "4. What the legal name of the user is\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e88676-0332-4cae-b505-53b0ec2e1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages_info(messages):\n",
    "    return [SystemMessage(content=template)] + messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714a439-0d0c-4f57-b965-9a0110136b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_chain(state):\n",
    "    messages = get_messages_info(state[\"messages\"])\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c9832-d9db-4005-9e3a-240535ce9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de7dfa-c580-4b63-8b77-b5dafd62f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def book_flight(from_city: str, to_city: str, travel_date: str, passenger_name: str) -> str:\n",
    "    \"\"\"Book a flight for the customer. Call this whenever you need to book a flight, for example when a customer asks 'I want to book a flight from Los Angeles to New York'\n",
    "    Args:\n",
    "        from_city: The departure city\n",
    "        to_city: The arrival city\n",
    "        travel_date: The date of travel\n",
    "        passenger_name: The passenger's legal name.\n",
    "    \"\"\"\n",
    "    output_json = { \"from_city\": from_city, \"to_city\": to_city, \"travel_date\": travel_date, \"passenger_name\": passenger_name }\n",
    "    print(\"book_flight is called\")\n",
    "    return json.dumps(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5be77a-81bf-4b44-88fb-17f3ae0a3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_tools(state) -> Literal[\"tools\", \"end\"]:\n",
    "    \"\"\"\n",
    "    Determine whether to continue to tools or end.\n",
    "    \n",
    "    This function checks if the last message has tool calls.\n",
    "    If yes, route to tools node. If no, end the conversation.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If there are tool calls, continue to tools node\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        return \"end\"    \n",
    "    # Otherwise, end the conversation\n",
    "    return \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1755ef0-78c6-49cf-8470-16ef7f2acf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = ChatOllama(model=\"gemma3:4b\", base_url=\"http://10.8.4.240:11434\")    \n",
    "llm = ChatOllama(model=\"llama3.1:8b\", base_url=\"http://10.8.4.240:11434\")    \n",
    "tools = [book_flight]\n",
    "llm_with_tool = llm.bind_tools(tools)\n",
    "#llm_with_tool = create_agent(llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ff0f7-8107-420f-b198-f199e1029255",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = InMemorySaver()\n",
    "workflow = StateGraph(State)\n",
    "# Add nodes\n",
    "workflow.add_node(\"info\", info_chain)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"info\")\n",
    "workflow.add_conditional_edges(\"info\", route_tools,{\"tools\":\"tools\",\"end\": END,\"info\":\"info\"})\n",
    "workflow.add_edge(\"tools\", END)\n",
    "# Compile\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6056bbf-22eb-4658-bb50-25343d927c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
    "cached_response_index = 0\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        user = cached_human_responses[cached_response_index]\n",
    "        cached_response_index += 1\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Bye bye\")\n",
    "        break\n",
    "    output   = None\n",
    "    set_exit = False\n",
    "    for output in graph.stream( {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\" ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        if hasattr(last_message, \"content\"):\n",
    "            if isinstance(last_message.content, list):\n",
    "                texts = [part.get(\"text\", \"\") for part in last_message.content if isinstance(part, dict) and \"text\" in part]\n",
    "                print(\"AI:\", \" \".join(texts))\n",
    "            else:\n",
    "                print(\"AI:\", last_message.content)\n",
    "        else:\n",
    "            print(\"AI:\", last_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06846965-a0d9-475b-80da-f58e6a9e06e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
